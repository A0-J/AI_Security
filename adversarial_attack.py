# -*- coding: utf-8 -*-
"""adversarial attack.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-NsOOBA2K0UNtGceMOMn0DNS2_cz8Eep
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import os

# CNN 모델 정의
class mnistNet(nn.Module):
    def __init__(self):
        super(mnistNet, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1)
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1)
        self.dropout1 = nn.Dropout2d(p=0.25)
        self.fc1 = nn.Linear(in_features=9216, out_features=128)
        self.dropout2 = nn.Dropout2d(p=0.5)
        self.fc2 = nn.Linear(in_features=128, out_features=10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        output = F.log_softmax(x, dim=1)
        return output

# 데이터 로드 함수
def load_data(train=True):
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    dataset = datasets.MNIST('./data', train=train, download=True, transform=transform)
    loader = torch.utils.data.DataLoader(dataset, batch_size=64 if train else 1, shuffle=True)
    return loader

# 모델 학습 및 저장 함수
def train_and_save_model(filepath, device):
    train_loader = load_data(train=True)
    model = mnistNet().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    loss_fn = nn.CrossEntropyLoss()

    print("Training the model...")
    model.train()
    for epoch in range(5):
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = model(data)
            loss = loss_fn(output, target)
            loss.backward()
            optimizer.step()
    torch.save(model.state_dict(), filepath)
    print(f"Model saved to {filepath}")

# 모델 로드 또는 학습
def load_or_train_model(filepath, device):
    if not os.path.exists(filepath):
        print(f"Model file {filepath} not found. Training a new model...")
        train_and_save_model(filepath, device)
    model = mnistNet().to(device)
    model.load_state_dict(torch.load(filepath, map_location=device))
    print(f"Model loaded successfully from {filepath}")
    return model

# FGSM 공격 함수
def fgsm_attack(model, device, data, target, epsilon):
    data.requires_grad = True
    output = model(data)
    loss = F.nll_loss(output, target)
    model.zero_grad()
    loss.backward()
    data_grad = data.grad.data
    perturbed_data = data + epsilon * data_grad.sign()
    perturbed_data = torch.clamp(perturbed_data, 0, 1)
    return perturbed_data

# FGSM 테스트 함수
def test_fgsm(model, device, test_loader, epsilon):
    correct = 0
    total = 0
    for data, target in test_loader:
        data, target = data.to(device), target.to(device)
        perturbed_data = fgsm_attack(model, device, data, target, epsilon)
        output = model(perturbed_data)
        pred = output.max(1, keepdim=True)[1]
        correct += pred.eq(target.view_as(pred)).sum().item()
        total += 1

    accuracy = 100 * correct / total
    print(f"Epsilon: {epsilon}\tTest Accuracy: {accuracy:.2f}%")
    return accuracy

# 메인 함수
def main():
    model_path = "./lenet_mnist_model.pth"
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    test_loader = load_data(train=False)
    epsilons = [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]

    # 모델 로드 또는 학습
    model = load_or_train_model(model_path, device)
    model.eval()

    # FGSM 테스트
    accuracies = []
    for eps in epsilons:
        acc = test_fgsm(model, device, test_loader, eps)
        accuracies.append(acc)

    # 결과 시각화
    plt.figure(figsize=(6, 5))
    plt.plot(epsilons, accuracies, "o-", label="FGSM Accuracy")
    plt.title("FGSM: Accuracy vs Epsilon")
    plt.xlabel("Epsilon")
    plt.ylabel("Accuracy (%)")
    plt.grid()
    plt.legend()
    plt.show()

if __name__ == "__main__":
    main()

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import os

# CNN 모델 정의
class mnistNet(nn.Module):
    def __init__(self):
        super(mnistNet, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1)
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1)
        self.dropout1 = nn.Dropout2d(p=0.25)
        self.fc1 = nn.Linear(in_features=9216, out_features=128)
        self.dropout2 = nn.Dropout2d(p=0.5)
        self.fc2 = nn.Linear(in_features=128, out_features=10)

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, 2)
        x = self.dropout1(x)
        x = torch.flatten(x, 1)
        x = self.fc1(x)
        x = F.relu(x)
        self.dropout2(x)
        x = self.fc2(x)
        output = F.log_softmax(x, dim=1)
        return output

# 데이터 로드 및 전처리
def load_data(train=True, batch_size=64):
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    dataset = datasets.MNIST('./data', train=train, download=True, transform=transform)
    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)
    return loader

# 모델 학습 및 저장
def train_and_save_model(filepath, device):
    train_loader = load_data(train=True)
    model = mnistNet().to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    loss_fn = nn.CrossEntropyLoss()

    print("Training the model...")
    model.train()
    epochs = 5
    for epoch in range(epochs):
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = model(data)
            loss = loss_fn(output, target)
            loss.backward()
            optimizer.step()
            if batch_idx % 100 == 0:
                print(f"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.6f}")

    # 모델 저장
    torch.save(model.state_dict(), filepath)
    print(f"Model saved to {filepath}")

# 모델 로드 또는 학습 함수
def load_or_train_model(filepath, device):
    if not os.path.exists(filepath):
        print(f"Model file {filepath} not found. Training a new model...")
        train_and_save_model(filepath, device)
    model = mnistNet().to(device)
    model.load_state_dict(torch.load(filepath, map_location=device))
    print(f"Model loaded successfully from {filepath}")
    return model

# PGD 공격 함수
def pgd_attack(model, images, labels, device, epsilon=0.3, alpha=0.01, iters=10):  # 반복 횟수 감소
    images = images.clone().detach().to(device)
    labels = labels.to(device)
    ori_images = images.clone().detach()
    loss = nn.CrossEntropyLoss()

    for _ in range(iters):
        images.requires_grad = True
        outputs = model(images)
        model.zero_grad()
        cost = loss(outputs, labels).to(device)
        cost.backward()
        adv_images = images + alpha * images.grad.sign()
        eta = torch.clamp(adv_images - ori_images, min=-epsilon, max=epsilon)
        images = torch.clamp(ori_images + eta, min=0, max=1).detach()

    return images

# PGD 테스트 함수
def test_pgd(model, device, test_loader, epsilon, alpha=0.01, iters=10):  # 반복 횟수 감소
    correct = 0
    total = 0
    for data, target in test_loader:
        data, target = data.to(device), target.to(device)
        perturbed_data = pgd_attack(model, data, target, device, epsilon, alpha, iters)
        output = model(perturbed_data)
        pred = output.max(1, keepdim=True)[1]
        correct += pred.eq(target.view_as(pred)).sum().item()
        total += len(target)

    accuracy = 100 * correct / total
    print(f"PGD Epsilon: {epsilon}\tTest Accuracy: {accuracy:.2f}%")
    return accuracy

# 메인 함수
def main():
    model_path = "./lenet_mnist_model.pth"
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    test_loader = load_data(train=False, batch_size=64)  # 배치 크기 증가
    epsilons = [0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3]

    # 모델 로드 또는 학습
    model = load_or_train_model(model_path, device)
    model.eval()

    # PGD 테스트
    accuracies = []
    for eps in epsilons:
        acc = test_pgd(model, device, test_loader, eps)
        accuracies.append(acc)

    # 결과 시각화
    plt.figure(figsize=(6, 5))
    plt.plot(epsilons, accuracies, "o-", label="PGD Accuracy")
    plt.title("PGD: Accuracy vs Epsilon")
    plt.xlabel("Epsilon")
    plt.ylabel("Accuracy (%)")
    plt.grid()
    plt.legend()
    plt.show()

# PGD 공격 테스트 함수 (Confidence Drop 추가)
def test_pgd_with_confidence(model, device, test_loader, epsilon, alpha=0.01, iters=40):
    correct = 0
    total = 0
    confidence_differences = []

    for data, target in test_loader:
        data, target = data.to(device), target.to(device)
        # 적대적 샘플 생성
        perturbed_data = pgd_attack(model, data, target, device, epsilon, alpha, iters)

        # 원본 및 적대적 샘플의 출력
        output_orig = model(data)
        output_adv = model(perturbed_data)

        # Confidence 계산
        conf_orig = F.softmax(output_orig, dim=1).max(1)[0].item()  # 원본 Confidence
        conf_adv = F.softmax(output_adv, dim=1).max(1)[0].item()    # 적대적 샘플 Confidence
        confidence_differences.append(conf_orig - conf_adv)

        # 적대적 샘플의 예측 정확도
        pred = output_adv.max(1, keepdim=True)[1]
        correct += pred.eq(target.view_as(pred)).sum().item()
        total += 1

    # 평균 Confidence Drop 계산
    avg_conf_drop = sum(confidence_differences) / len(confidence_differences)
    accuracy = 100 * correct / total
    print(f"PGD Epsilon: {epsilon}\tTest Accuracy: {accuracy:.2f}%\tAvg Confidence Drop: {avg_conf_drop:.5f}")
    return accuracy, avg_conf_drop


if __name__ == "__main__":
    main()